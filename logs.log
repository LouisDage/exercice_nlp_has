2024-09-03 21:18:12,951:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-03 21:18:12,952:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-03 21:18:12,952:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-03 21:18:12,953:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 10:10:34,925:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 10:10:34,926:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 10:10:34,927:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 10:10:34,927:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 12:36:32,493:INFO:PyCaret ClassificationExperiment
2024-09-04 12:36:32,494:INFO:Logging name: clf-default-name
2024-09-04 12:36:32,495:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-04 12:36:32,496:INFO:version 3.3.2
2024-09-04 12:36:32,496:INFO:Initializing setup()
2024-09-04 12:36:32,497:INFO:self.USI: 96f5
2024-09-04 12:36:32,498:INFO:self._variable_keys: {'idx', 'fix_imbalance', 'seed', 'exp_name_log', 'y', 'y_test', 'fold_shuffle_param', 'exp_id', 'USI', 'logging_param', '_ml_usecase', 'target_param', 'memory', 'n_jobs_param', 'pipeline', 'fold_groups_param', 'X_test', 'gpu_param', 'gpu_n_jobs_param', 'html_param', 'y_train', 'is_multiclass', 'fold_generator', '_available_plots', 'X_train', 'log_plots_param', 'data', 'X'}
2024-09-04 12:36:32,499:INFO:Checking environment
2024-09-04 12:36:32,500:INFO:python_version: 3.9.19
2024-09-04 12:36:32,501:INFO:python_build: ('main', 'Mar 20 2024 12:50:21')
2024-09-04 12:36:32,502:INFO:machine: x86_64
2024-09-04 12:36:32,503:INFO:platform: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
2024-09-04 12:36:32,504:INFO:Memory: svmem(total=33565290496, available=15909634048, percent=52.6, used=17095786496, free=14758080512, active=890941440, inactive=16720470016, buffers=39620608, cached=1671802880, shared=88346624, slab=548782080)
2024-09-04 12:36:32,506:INFO:Physical Core: 10
2024-09-04 12:36:32,507:INFO:Logical Core: 20
2024-09-04 12:36:32,507:INFO:Checking libraries
2024-09-04 12:36:32,508:INFO:System:
2024-09-04 12:36:32,509:INFO:    python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21)  [GCC 12.3.0]
2024-09-04 12:36:32,509:INFO:executable: /home/default/miniconda/envs/BMA/bin/python
2024-09-04 12:36:32,510:INFO:   machine: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
2024-09-04 12:36:32,511:INFO:PyCaret required dependencies:
2024-09-04 12:36:32,634:INFO:                 pip: 24.0
2024-09-04 12:36:32,634:INFO:          setuptools: 70.0.0
2024-09-04 12:36:32,635:INFO:             pycaret: 3.3.2
2024-09-04 12:36:32,635:INFO:             IPython: 8.18.1
2024-09-04 12:36:32,635:INFO:          ipywidgets: 8.1.5
2024-09-04 12:36:32,636:INFO:                tqdm: 4.66.4
2024-09-04 12:36:32,636:INFO:               numpy: 1.26.3
2024-09-04 12:36:32,637:INFO:              pandas: 2.1.4
2024-09-04 12:36:32,637:INFO:              jinja2: 3.1.3
2024-09-04 12:36:32,637:INFO:               scipy: 1.11.4
2024-09-04 12:36:32,638:INFO:              joblib: 1.3.2
2024-09-04 12:36:32,638:INFO:             sklearn: 1.4.2
2024-09-04 12:36:32,638:INFO:                pyod: 2.0.1
2024-09-04 12:36:32,639:INFO:            imblearn: 0.12.3
2024-09-04 12:36:32,639:INFO:   category_encoders: 2.6.3
2024-09-04 12:36:32,639:INFO:            lightgbm: 4.5.0
2024-09-04 12:36:32,640:INFO:               numba: 0.60.0
2024-09-04 12:36:32,640:INFO:            requests: 2.32.3
2024-09-04 12:36:32,641:INFO:          matplotlib: 3.7.5
2024-09-04 12:36:32,641:INFO:          scikitplot: 0.3.7
2024-09-04 12:36:32,642:INFO:         yellowbrick: 1.5
2024-09-04 12:36:32,642:INFO:              plotly: 5.22.0
2024-09-04 12:36:32,643:INFO:    plotly-resampler: Not installed
2024-09-04 12:36:32,643:INFO:             kaleido: 0.2.1
2024-09-04 12:36:32,643:INFO:           schemdraw: 0.15
2024-09-04 12:36:32,644:INFO:         statsmodels: 0.14.2
2024-09-04 12:36:32,644:INFO:              sktime: 0.26.0
2024-09-04 12:36:32,645:INFO:               tbats: 1.1.3
2024-09-04 12:36:32,645:INFO:            pmdarima: 2.0.4
2024-09-04 12:36:32,645:INFO:              psutil: 5.9.8
2024-09-04 12:36:32,646:INFO:          markupsafe: 2.1.5
2024-09-04 12:36:32,646:INFO:             pickle5: Not installed
2024-09-04 12:36:32,646:INFO:         cloudpickle: 3.0.0
2024-09-04 12:36:32,647:INFO:         deprecation: 2.1.0
2024-09-04 12:36:32,647:INFO:              xxhash: 3.4.1
2024-09-04 12:36:32,647:INFO:           wurlitzer: 3.1.1
2024-09-04 12:36:32,647:INFO:PyCaret optional dependencies:
2024-09-04 12:36:33,080:INFO:                shap: Not installed
2024-09-04 12:36:33,080:INFO:           interpret: Not installed
2024-09-04 12:36:33,081:INFO:                umap: Not installed
2024-09-04 12:36:33,082:INFO:     ydata_profiling: Not installed
2024-09-04 12:36:33,082:INFO:  explainerdashboard: Not installed
2024-09-04 12:36:33,083:INFO:             autoviz: Not installed
2024-09-04 12:36:33,083:INFO:           fairlearn: Not installed
2024-09-04 12:36:33,084:INFO:          deepchecks: Not installed
2024-09-04 12:36:33,085:INFO:             xgboost: Not installed
2024-09-04 12:36:33,085:INFO:            catboost: Not installed
2024-09-04 12:36:33,086:INFO:              kmodes: Not installed
2024-09-04 12:36:33,087:INFO:             mlxtend: Not installed
2024-09-04 12:36:33,087:INFO:       statsforecast: Not installed
2024-09-04 12:36:33,088:INFO:        tune_sklearn: Not installed
2024-09-04 12:36:33,088:INFO:                 ray: Not installed
2024-09-04 12:36:33,089:INFO:            hyperopt: Not installed
2024-09-04 12:36:33,090:INFO:              optuna: Not installed
2024-09-04 12:36:33,090:INFO:               skopt: Not installed
2024-09-04 12:36:33,091:INFO:              mlflow: 2.14.0
2024-09-04 12:36:33,091:INFO:              gradio: Not installed
2024-09-04 12:36:33,092:INFO:             fastapi: 0.111.0
2024-09-04 12:36:33,093:INFO:             uvicorn: 0.30.1
2024-09-04 12:36:33,093:INFO:              m2cgen: Not installed
2024-09-04 12:36:33,094:INFO:           evidently: Not installed
2024-09-04 12:36:33,095:INFO:               fugue: Not installed
2024-09-04 12:36:33,095:INFO:           streamlit: 1.36.0
2024-09-04 12:36:33,096:INFO:             prophet: Not installed
2024-09-04 12:36:33,096:INFO:None
2024-09-04 12:36:33,097:INFO:Set up data.
2024-09-04 12:36:34,284:INFO:Set up folding strategy.
2024-09-04 12:36:34,285:INFO:Set up train/test split.
2024-09-04 12:36:34,340:INFO:Set up index.
2024-09-04 12:36:34,342:INFO:Assigning column types.
2024-09-04 12:36:34,405:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-04 12:36:34,435:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-04 12:36:34,439:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 12:36:34,465:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 12:36:34,465:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 12:36:34,496:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-04 12:36:34,497:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 12:36:34,515:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 12:36:34,516:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 12:36:34,517:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-04 12:36:34,551:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 12:36:34,572:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 12:36:34,573:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 12:36:34,604:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 12:36:34,622:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 12:36:34,623:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 12:36:34,624:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-04 12:36:34,670:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 12:36:34,671:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 12:36:34,723:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 12:36:34,724:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 12:36:34,727:INFO:Preparing preprocessing pipeline...
2024-09-04 12:36:34,740:INFO:Set up simple imputation.
2024-09-04 12:36:34,907:INFO:Finished creating preprocessing pipeline.
2024-09-04 12:36:34,912:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-09-04 12:36:34,913:INFO:Creating final display dataframe.
2024-09-04 12:36:35,471:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target         Class_Int
2                   Target type        Multiclass
3           Original data shape       (7716, 385)
4        Transformed data shape       (7716, 385)
5   Transformed train set shape       (5401, 385)
6    Transformed test set shape       (2315, 385)
7              Numeric features               384
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              96f5
2024-09-04 12:36:35,538:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 12:36:35,539:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 12:36:35,598:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 12:36:35,599:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 12:36:35,601:INFO:setup() successfully completed in 3.14s...............
2024-09-04 12:37:16,464:INFO:Initializing compare_models()
2024-09-04 12:37:16,467:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd6b8d6f670>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7fd6b8d6f670>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-09-04 12:37:16,471:INFO:Checking exceptions
2024-09-04 12:37:16,601:INFO:Preparing display monitor
2024-09-04 12:37:16,641:INFO:Initializing Logistic Regression
2024-09-04 12:37:16,642:INFO:Total runtime is 1.0712941487630208e-05 minutes
2024-09-04 12:37:16,650:INFO:SubProcess create_model() called ==================================
2024-09-04 12:37:16,652:INFO:Initializing create_model()
2024-09-04 12:37:16,653:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd6b8d6f670>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd69d28e880>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 12:37:16,653:INFO:Checking exceptions
2024-09-04 12:37:16,654:INFO:Importing libraries
2024-09-04 12:37:16,654:INFO:Copying training dataset
2024-09-04 12:37:16,771:INFO:Defining folds
2024-09-04 12:37:16,772:INFO:Declaring metric variables
2024-09-04 12:37:16,779:INFO:Importing untrained model
2024-09-04 12:37:16,789:INFO:Logistic Regression Imported successfully
2024-09-04 12:37:16,806:INFO:Starting cross validation
2024-09-04 12:37:16,809:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 12:37:16,859:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:16,861:WARNING:To disable this warning, you can either:
2024-09-04 12:37:16,862:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:16,863:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:16,908:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:16,926:WARNING:To disable this warning, you can either:
2024-09-04 12:37:16,951:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:16,953:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:16,955:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:16,957:WARNING:To disable this warning, you can either:
2024-09-04 12:37:16,981:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:16,983:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:17,011:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:17,039:WARNING:To disable this warning, you can either:
2024-09-04 12:37:17,042:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:17,044:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:17,077:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:17,112:WARNING:To disable this warning, you can either:
2024-09-04 12:37:17,114:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:17,115:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:17,143:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:17,145:WARNING:To disable this warning, you can either:
2024-09-04 12:37:17,147:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:17,172:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:17,173:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:17,175:WARNING:To disable this warning, you can either:
2024-09-04 12:37:17,175:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:17,203:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:17,204:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:17,207:WARNING:To disable this warning, you can either:
2024-09-04 12:37:17,236:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:17,240:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:17,278:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:17,280:WARNING:To disable this warning, you can either:
2024-09-04 12:37:17,281:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:17,282:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:17,309:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:17,312:WARNING:To disable this warning, you can either:
2024-09-04 12:37:17,313:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:17,314:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:17,347:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:17,348:WARNING:To disable this warning, you can either:
2024-09-04 12:37:17,349:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:17,350:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:17,376:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:17,378:WARNING:To disable this warning, you can either:
2024-09-04 12:37:17,379:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:17,412:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:17,415:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:17,445:WARNING:To disable this warning, you can either:
2024-09-04 12:37:17,449:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:17,449:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:17,452:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:17,480:WARNING:To disable this warning, you can either:
2024-09-04 12:37:17,481:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:17,483:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:17,529:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:17,531:WARNING:To disable this warning, you can either:
2024-09-04 12:37:17,533:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:17,534:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:17,562:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:17,568:WARNING:To disable this warning, you can either:
2024-09-04 12:37:17,571:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:17,572:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:17,577:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:17,584:WARNING:To disable this warning, you can either:
2024-09-04 12:37:17,590:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:17,599:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:17,605:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:17,610:WARNING:To disable this warning, you can either:
2024-09-04 12:37:17,611:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:17,611:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:17,612:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:17,612:WARNING:To disable this warning, you can either:
2024-09-04 12:37:17,613:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:17,613:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:17,614:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:17,614:WARNING:To disable this warning, you can either:
2024-09-04 12:37:17,615:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:17,616:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:17,616:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:17,617:WARNING:To disable this warning, you can either:
2024-09-04 12:37:17,618:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:17,618:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:17,619:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:37:17,620:WARNING:To disable this warning, you can either:
2024-09-04 12:37:17,621:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:37:17,621:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:37:21,492:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:21,594:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:21,606:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:21,637:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:21,657:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:21,672:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:21,675:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:21,680:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:21,692:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:21,730:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:21,749:INFO:Calculating mean and std
2024-09-04 12:37:21,753:INFO:Creating metrics dataframe
2024-09-04 12:37:21,760:INFO:Uploading results into container
2024-09-04 12:37:21,768:INFO:Uploading model into container now
2024-09-04 12:37:21,774:INFO:_master_model_container: 1
2024-09-04 12:37:21,776:INFO:_display_container: 2
2024-09-04 12:37:21,779:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-04 12:37:21,780:INFO:create_model() successfully completed......................................
2024-09-04 12:37:22,310:INFO:SubProcess create_model() end ==================================
2024-09-04 12:37:22,312:INFO:Creating metrics dataframe
2024-09-04 12:37:22,343:INFO:Initializing K Neighbors Classifier
2024-09-04 12:37:22,344:INFO:Total runtime is 0.09504741430282593 minutes
2024-09-04 12:37:22,358:INFO:SubProcess create_model() called ==================================
2024-09-04 12:37:22,360:INFO:Initializing create_model()
2024-09-04 12:37:22,361:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd6b8d6f670>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd69d28e880>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 12:37:22,362:INFO:Checking exceptions
2024-09-04 12:37:22,363:INFO:Importing libraries
2024-09-04 12:37:22,365:INFO:Copying training dataset
2024-09-04 12:37:22,473:INFO:Defining folds
2024-09-04 12:37:22,474:INFO:Declaring metric variables
2024-09-04 12:37:22,479:INFO:Importing untrained model
2024-09-04 12:37:22,483:INFO:K Neighbors Classifier Imported successfully
2024-09-04 12:37:22,492:INFO:Starting cross validation
2024-09-04 12:37:22,494:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 12:37:25,097:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:25,102:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:25,287:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:25,385:INFO:Calculating mean and std
2024-09-04 12:37:25,387:INFO:Creating metrics dataframe
2024-09-04 12:37:25,391:INFO:Uploading results into container
2024-09-04 12:37:25,393:INFO:Uploading model into container now
2024-09-04 12:37:25,394:INFO:_master_model_container: 2
2024-09-04 12:37:25,395:INFO:_display_container: 2
2024-09-04 12:37:25,396:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-04 12:37:25,397:INFO:create_model() successfully completed......................................
2024-09-04 12:37:25,745:INFO:SubProcess create_model() end ==================================
2024-09-04 12:37:25,746:INFO:Creating metrics dataframe
2024-09-04 12:37:25,773:INFO:Initializing Naive Bayes
2024-09-04 12:37:25,774:INFO:Total runtime is 0.15220534801483154 minutes
2024-09-04 12:37:25,781:INFO:SubProcess create_model() called ==================================
2024-09-04 12:37:25,782:INFO:Initializing create_model()
2024-09-04 12:37:25,783:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd6b8d6f670>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd69d28e880>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 12:37:25,784:INFO:Checking exceptions
2024-09-04 12:37:25,785:INFO:Importing libraries
2024-09-04 12:37:25,787:INFO:Copying training dataset
2024-09-04 12:37:25,871:INFO:Defining folds
2024-09-04 12:37:25,871:INFO:Declaring metric variables
2024-09-04 12:37:25,875:INFO:Importing untrained model
2024-09-04 12:37:25,880:INFO:Naive Bayes Imported successfully
2024-09-04 12:37:25,887:INFO:Starting cross validation
2024-09-04 12:37:25,889:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 12:37:26,254:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:26,270:INFO:Calculating mean and std
2024-09-04 12:37:26,271:INFO:Creating metrics dataframe
2024-09-04 12:37:26,274:INFO:Uploading results into container
2024-09-04 12:37:26,275:INFO:Uploading model into container now
2024-09-04 12:37:26,275:INFO:_master_model_container: 3
2024-09-04 12:37:26,276:INFO:_display_container: 2
2024-09-04 12:37:26,278:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-04 12:37:26,279:INFO:create_model() successfully completed......................................
2024-09-04 12:37:26,533:INFO:SubProcess create_model() end ==================================
2024-09-04 12:37:26,534:INFO:Creating metrics dataframe
2024-09-04 12:37:26,550:INFO:Initializing Decision Tree Classifier
2024-09-04 12:37:26,551:INFO:Total runtime is 0.1651590387026469 minutes
2024-09-04 12:37:26,559:INFO:SubProcess create_model() called ==================================
2024-09-04 12:37:26,561:INFO:Initializing create_model()
2024-09-04 12:37:26,561:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd6b8d6f670>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd69d28e880>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 12:37:26,562:INFO:Checking exceptions
2024-09-04 12:37:26,563:INFO:Importing libraries
2024-09-04 12:37:26,563:INFO:Copying training dataset
2024-09-04 12:37:26,642:INFO:Defining folds
2024-09-04 12:37:26,643:INFO:Declaring metric variables
2024-09-04 12:37:26,646:INFO:Importing untrained model
2024-09-04 12:37:26,651:INFO:Decision Tree Classifier Imported successfully
2024-09-04 12:37:26,658:INFO:Starting cross validation
2024-09-04 12:37:26,660:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 12:37:30,260:INFO:Calculating mean and std
2024-09-04 12:37:30,266:INFO:Creating metrics dataframe
2024-09-04 12:37:30,270:INFO:Uploading results into container
2024-09-04 12:37:30,272:INFO:Uploading model into container now
2024-09-04 12:37:30,276:INFO:_master_model_container: 4
2024-09-04 12:37:30,277:INFO:_display_container: 2
2024-09-04 12:37:30,282:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-04 12:37:30,283:INFO:create_model() successfully completed......................................
2024-09-04 12:37:30,567:INFO:SubProcess create_model() end ==================================
2024-09-04 12:37:30,569:INFO:Creating metrics dataframe
2024-09-04 12:37:30,591:INFO:Initializing SVM - Linear Kernel
2024-09-04 12:37:30,592:INFO:Total runtime is 0.23251251379648846 minutes
2024-09-04 12:37:30,603:INFO:SubProcess create_model() called ==================================
2024-09-04 12:37:30,607:INFO:Initializing create_model()
2024-09-04 12:37:30,609:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd6b8d6f670>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd69d28e880>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 12:37:30,612:INFO:Checking exceptions
2024-09-04 12:37:30,613:INFO:Importing libraries
2024-09-04 12:37:30,615:INFO:Copying training dataset
2024-09-04 12:37:30,705:INFO:Defining folds
2024-09-04 12:37:30,706:INFO:Declaring metric variables
2024-09-04 12:37:30,710:INFO:Importing untrained model
2024-09-04 12:37:30,715:INFO:SVM - Linear Kernel Imported successfully
2024-09-04 12:37:30,724:INFO:Starting cross validation
2024-09-04 12:37:30,725:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 12:37:30,994:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:31,048:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:31,136:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:31,189:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:31,274:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:31,297:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:31,379:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:31,392:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:31,441:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:31,451:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:31,465:INFO:Calculating mean and std
2024-09-04 12:37:31,467:INFO:Creating metrics dataframe
2024-09-04 12:37:31,472:INFO:Uploading results into container
2024-09-04 12:37:31,473:INFO:Uploading model into container now
2024-09-04 12:37:31,475:INFO:_master_model_container: 5
2024-09-04 12:37:31,476:INFO:_display_container: 2
2024-09-04 12:37:31,478:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-04 12:37:31,479:INFO:create_model() successfully completed......................................
2024-09-04 12:37:31,894:INFO:SubProcess create_model() end ==================================
2024-09-04 12:37:31,894:INFO:Creating metrics dataframe
2024-09-04 12:37:31,911:INFO:Initializing Ridge Classifier
2024-09-04 12:37:31,912:INFO:Total runtime is 0.25450566212336223 minutes
2024-09-04 12:37:31,928:INFO:SubProcess create_model() called ==================================
2024-09-04 12:37:31,930:INFO:Initializing create_model()
2024-09-04 12:37:31,933:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd6b8d6f670>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd69d28e880>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 12:37:31,933:INFO:Checking exceptions
2024-09-04 12:37:31,934:INFO:Importing libraries
2024-09-04 12:37:31,935:INFO:Copying training dataset
2024-09-04 12:37:32,005:INFO:Defining folds
2024-09-04 12:37:32,005:INFO:Declaring metric variables
2024-09-04 12:37:32,010:INFO:Importing untrained model
2024-09-04 12:37:32,015:INFO:Ridge Classifier Imported successfully
2024-09-04 12:37:32,024:INFO:Starting cross validation
2024-09-04 12:37:32,026:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 12:37:32,257:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:32,273:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:32,303:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:32,308:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:32,344:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:32,369:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:32,376:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:32,406:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:32,413:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:32,457:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:32,464:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:32,498:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:32,498:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:32,504:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:32,530:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:32,536:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:32,542:INFO:Calculating mean and std
2024-09-04 12:37:32,543:INFO:Creating metrics dataframe
2024-09-04 12:37:32,545:INFO:Uploading results into container
2024-09-04 12:37:32,546:INFO:Uploading model into container now
2024-09-04 12:37:32,547:INFO:_master_model_container: 6
2024-09-04 12:37:32,547:INFO:_display_container: 2
2024-09-04 12:37:32,549:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-04 12:37:32,549:INFO:create_model() successfully completed......................................
2024-09-04 12:37:32,895:INFO:SubProcess create_model() end ==================================
2024-09-04 12:37:32,896:INFO:Creating metrics dataframe
2024-09-04 12:37:32,907:INFO:Initializing Random Forest Classifier
2024-09-04 12:37:32,907:INFO:Total runtime is 0.2710989236831665 minutes
2024-09-04 12:37:32,912:INFO:SubProcess create_model() called ==================================
2024-09-04 12:37:32,912:INFO:Initializing create_model()
2024-09-04 12:37:32,913:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd6b8d6f670>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd69d28e880>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 12:37:32,913:INFO:Checking exceptions
2024-09-04 12:37:32,914:INFO:Importing libraries
2024-09-04 12:37:32,914:INFO:Copying training dataset
2024-09-04 12:37:32,988:INFO:Defining folds
2024-09-04 12:37:32,989:INFO:Declaring metric variables
2024-09-04 12:37:32,993:INFO:Importing untrained model
2024-09-04 12:37:32,998:INFO:Random Forest Classifier Imported successfully
2024-09-04 12:37:33,008:INFO:Starting cross validation
2024-09-04 12:37:33,010:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 12:37:40,608:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:40,628:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:40,644:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:40,653:INFO:Calculating mean and std
2024-09-04 12:37:40,655:INFO:Creating metrics dataframe
2024-09-04 12:37:40,659:INFO:Uploading results into container
2024-09-04 12:37:40,660:INFO:Uploading model into container now
2024-09-04 12:37:40,661:INFO:_master_model_container: 7
2024-09-04 12:37:40,662:INFO:_display_container: 2
2024-09-04 12:37:40,662:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-04 12:37:40,663:INFO:create_model() successfully completed......................................
2024-09-04 12:37:40,923:INFO:SubProcess create_model() end ==================================
2024-09-04 12:37:40,924:INFO:Creating metrics dataframe
2024-09-04 12:37:40,938:INFO:Initializing Quadratic Discriminant Analysis
2024-09-04 12:37:40,939:INFO:Total runtime is 0.40495950778325396 minutes
2024-09-04 12:37:40,943:INFO:SubProcess create_model() called ==================================
2024-09-04 12:37:40,944:INFO:Initializing create_model()
2024-09-04 12:37:40,944:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd6b8d6f670>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd69d28e880>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 12:37:40,945:INFO:Checking exceptions
2024-09-04 12:37:40,945:INFO:Importing libraries
2024-09-04 12:37:40,945:INFO:Copying training dataset
2024-09-04 12:37:41,008:INFO:Defining folds
2024-09-04 12:37:41,009:INFO:Declaring metric variables
2024-09-04 12:37:41,014:INFO:Importing untrained model
2024-09-04 12:37:41,020:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-04 12:37:41,034:INFO:Starting cross validation
2024-09-04 12:37:41,038:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 12:37:41,564:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 12:37:41,589:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 12:37:41,625:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 12:37:41,729:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 12:37:41,747:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 12:37:41,789:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 12:37:41,846:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 12:37:41,871:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 12:37:41,917:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 12:37:41,937:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 12:37:42,005:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:42,008:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:42,013:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:42,015:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:42,060:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:42,067:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:42,098:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:42,107:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:42,144:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:42,148:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:42,175:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:42,180:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:42,208:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:42,218:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:42,221:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:42,225:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:42,235:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:42,239:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:42,298:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:42,302:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:42,310:INFO:Calculating mean and std
2024-09-04 12:37:42,311:INFO:Creating metrics dataframe
2024-09-04 12:37:42,314:INFO:Uploading results into container
2024-09-04 12:37:42,315:INFO:Uploading model into container now
2024-09-04 12:37:42,315:INFO:_master_model_container: 8
2024-09-04 12:37:42,316:INFO:_display_container: 2
2024-09-04 12:37:42,318:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-04 12:37:42,321:INFO:create_model() successfully completed......................................
2024-09-04 12:37:42,557:INFO:SubProcess create_model() end ==================================
2024-09-04 12:37:42,558:INFO:Creating metrics dataframe
2024-09-04 12:37:42,570:INFO:Initializing Ada Boost Classifier
2024-09-04 12:37:42,570:INFO:Total runtime is 0.432148540019989 minutes
2024-09-04 12:37:42,578:INFO:SubProcess create_model() called ==================================
2024-09-04 12:37:42,584:INFO:Initializing create_model()
2024-09-04 12:37:42,585:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd6b8d6f670>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd69d28e880>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 12:37:42,586:INFO:Checking exceptions
2024-09-04 12:37:42,587:INFO:Importing libraries
2024-09-04 12:37:42,587:INFO:Copying training dataset
2024-09-04 12:37:42,710:INFO:Defining folds
2024-09-04 12:37:42,711:INFO:Declaring metric variables
2024-09-04 12:37:42,714:INFO:Importing untrained model
2024-09-04 12:37:42,721:INFO:Ada Boost Classifier Imported successfully
2024-09-04 12:37:42,727:INFO:Starting cross validation
2024-09-04 12:37:42,729:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 12:37:42,827:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 12:37:42,866:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 12:37:42,883:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 12:37:42,915:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 12:37:42,931:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 12:37:42,963:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 12:37:43,015:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 12:37:43,020:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 12:37:43,031:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 12:37:43,059:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 12:37:57,073:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:57,082:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:57,221:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:57,229:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:57,249:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:57,258:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:57,379:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:57,386:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:57,387:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:57,391:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:57,447:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:57,520:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:57,524:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:57,596:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:57,650:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:57,654:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:57,742:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:37:57,746:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:37:57,759:INFO:Calculating mean and std
2024-09-04 12:37:57,761:INFO:Creating metrics dataframe
2024-09-04 12:37:57,763:INFO:Uploading results into container
2024-09-04 12:37:57,764:INFO:Uploading model into container now
2024-09-04 12:37:57,765:INFO:_master_model_container: 9
2024-09-04 12:37:57,766:INFO:_display_container: 2
2024-09-04 12:37:57,766:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-04 12:37:57,766:INFO:create_model() successfully completed......................................
2024-09-04 12:37:57,970:INFO:SubProcess create_model() end ==================================
2024-09-04 12:37:57,971:INFO:Creating metrics dataframe
2024-09-04 12:37:57,979:INFO:Initializing Gradient Boosting Classifier
2024-09-04 12:37:57,979:INFO:Total runtime is 0.6889610727628072 minutes
2024-09-04 12:37:57,983:INFO:SubProcess create_model() called ==================================
2024-09-04 12:37:57,984:INFO:Initializing create_model()
2024-09-04 12:37:57,984:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd6b8d6f670>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd69d28e880>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 12:37:57,984:INFO:Checking exceptions
2024-09-04 12:37:57,985:INFO:Importing libraries
2024-09-04 12:37:57,985:INFO:Copying training dataset
2024-09-04 12:37:58,045:INFO:Defining folds
2024-09-04 12:37:58,045:INFO:Declaring metric variables
2024-09-04 12:37:58,049:INFO:Importing untrained model
2024-09-04 12:37:58,053:INFO:Gradient Boosting Classifier Imported successfully
2024-09-04 12:37:58,059:INFO:Starting cross validation
2024-09-04 12:37:58,060:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 12:51:40,872:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:51:41,380:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:51:41,592:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:51:44,313:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:51:46,092:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:51:46,140:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:51:48,852:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:51:48,856:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:51:50,008:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:51:51,545:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:51:51,851:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:51:51,875:INFO:Calculating mean and std
2024-09-04 12:51:51,879:INFO:Creating metrics dataframe
2024-09-04 12:51:51,887:INFO:Uploading results into container
2024-09-04 12:51:51,889:INFO:Uploading model into container now
2024-09-04 12:51:51,892:INFO:_master_model_container: 10
2024-09-04 12:51:51,893:INFO:_display_container: 2
2024-09-04 12:51:51,896:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-04 12:51:51,897:INFO:create_model() successfully completed......................................
2024-09-04 12:51:52,344:INFO:SubProcess create_model() end ==================================
2024-09-04 12:51:52,346:INFO:Creating metrics dataframe
2024-09-04 12:51:52,369:INFO:Initializing Linear Discriminant Analysis
2024-09-04 12:51:52,370:INFO:Total runtime is 14.595474358399708 minutes
2024-09-04 12:51:52,377:INFO:SubProcess create_model() called ==================================
2024-09-04 12:51:52,377:INFO:Initializing create_model()
2024-09-04 12:51:52,378:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd6b8d6f670>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd69d28e880>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 12:51:52,379:INFO:Checking exceptions
2024-09-04 12:51:52,380:INFO:Importing libraries
2024-09-04 12:51:52,381:INFO:Copying training dataset
2024-09-04 12:51:52,447:INFO:Defining folds
2024-09-04 12:51:52,447:INFO:Declaring metric variables
2024-09-04 12:51:52,451:INFO:Importing untrained model
2024-09-04 12:51:52,457:INFO:Linear Discriminant Analysis Imported successfully
2024-09-04 12:51:52,464:INFO:Starting cross validation
2024-09-04 12:51:52,467:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 12:51:52,508:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:51:52,527:WARNING:To disable this warning, you can either:
2024-09-04 12:51:52,529:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:51:52,529:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:51:52,550:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:51:52,551:WARNING:To disable this warning, you can either:
2024-09-04 12:51:52,553:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:51:52,554:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:51:52,573:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:51:52,575:WARNING:To disable this warning, you can either:
2024-09-04 12:51:52,596:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:51:52,598:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:51:52,621:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:51:52,623:WARNING:To disable this warning, you can either:
2024-09-04 12:51:52,624:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:51:52,646:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:51:52,648:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:51:52,649:WARNING:To disable this warning, you can either:
2024-09-04 12:51:52,652:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:51:52,675:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:51:52,698:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:51:52,700:WARNING:To disable this warning, you can either:
2024-09-04 12:51:52,702:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:51:52,724:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:51:52,725:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:51:52,726:WARNING:To disable this warning, you can either:
2024-09-04 12:51:52,732:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:51:52,744:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:51:52,750:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:51:52,756:WARNING:To disable this warning, you can either:
2024-09-04 12:51:52,757:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:51:52,757:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:51:52,758:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:51:52,759:WARNING:To disable this warning, you can either:
2024-09-04 12:51:52,759:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:51:52,760:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:51:52,760:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 12:51:52,762:WARNING:To disable this warning, you can either:
2024-09-04 12:51:52,762:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 12:51:52,763:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 12:51:52,945:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:51:53,264:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:51:53,295:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:51:53,352:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:51:53,391:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:51:53,399:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:51:53,439:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:51:53,465:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:51:53,491:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:51:53,512:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 12:51:53,525:INFO:Calculating mean and std
2024-09-04 12:51:53,527:INFO:Creating metrics dataframe
2024-09-04 12:51:53,530:INFO:Uploading results into container
2024-09-04 12:51:53,531:INFO:Uploading model into container now
2024-09-04 12:51:53,533:INFO:_master_model_container: 11
2024-09-04 12:51:53,533:INFO:_display_container: 2
2024-09-04 12:51:53,534:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-04 12:51:53,535:INFO:create_model() successfully completed......................................
2024-09-04 12:51:53,861:INFO:SubProcess create_model() end ==================================
2024-09-04 12:51:53,862:INFO:Creating metrics dataframe
2024-09-04 12:51:53,882:INFO:Initializing Extra Trees Classifier
2024-09-04 12:51:53,883:INFO:Total runtime is 14.620690878232319 minutes
2024-09-04 12:51:53,889:INFO:SubProcess create_model() called ==================================
2024-09-04 12:51:53,890:INFO:Initializing create_model()
2024-09-04 12:51:53,891:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd6b8d6f670>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd69d28e880>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 12:51:53,892:INFO:Checking exceptions
2024-09-04 12:51:53,892:INFO:Importing libraries
2024-09-04 12:51:53,893:INFO:Copying training dataset
2024-09-04 12:51:53,960:INFO:Defining folds
2024-09-04 12:51:53,960:INFO:Declaring metric variables
2024-09-04 12:51:53,964:INFO:Importing untrained model
2024-09-04 12:51:53,971:INFO:Extra Trees Classifier Imported successfully
2024-09-04 12:51:53,978:INFO:Starting cross validation
2024-09-04 12:51:53,980:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 12:51:57,421:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 12:51:57,497:INFO:Calculating mean and std
2024-09-04 12:51:57,499:INFO:Creating metrics dataframe
2024-09-04 12:51:57,501:INFO:Uploading results into container
2024-09-04 12:51:57,503:INFO:Uploading model into container now
2024-09-04 12:51:57,504:INFO:_master_model_container: 12
2024-09-04 12:51:57,504:INFO:_display_container: 2
2024-09-04 12:51:57,505:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-04 12:51:57,506:INFO:create_model() successfully completed......................................
2024-09-04 12:51:57,856:INFO:SubProcess create_model() end ==================================
2024-09-04 12:51:57,856:INFO:Creating metrics dataframe
2024-09-04 12:51:57,865:INFO:Initializing Light Gradient Boosting Machine
2024-09-04 12:51:57,866:INFO:Total runtime is 14.687075491746265 minutes
2024-09-04 12:51:57,871:INFO:SubProcess create_model() called ==================================
2024-09-04 12:51:57,872:INFO:Initializing create_model()
2024-09-04 12:51:57,873:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd6b8d6f670>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd69d28e880>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 12:51:57,874:INFO:Checking exceptions
2024-09-04 12:51:57,875:INFO:Importing libraries
2024-09-04 12:51:57,875:INFO:Copying training dataset
2024-09-04 12:51:57,943:INFO:Defining folds
2024-09-04 12:51:57,944:INFO:Declaring metric variables
2024-09-04 12:51:57,951:INFO:Importing untrained model
2024-09-04 12:51:57,966:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 12:51:57,977:INFO:Starting cross validation
2024-09-04 12:51:57,979:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 13:19:10,491:INFO:PyCaret ClassificationExperiment
2024-09-04 13:19:10,493:INFO:Logging name: clf-default-name
2024-09-04 13:19:10,495:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-04 13:19:10,496:INFO:version 3.3.2
2024-09-04 13:19:10,497:INFO:Initializing setup()
2024-09-04 13:19:10,498:INFO:self.USI: 3a5e
2024-09-04 13:19:10,499:INFO:self._variable_keys: {'idx', 'fix_imbalance', 'seed', 'exp_name_log', 'y', 'y_test', 'fold_shuffle_param', 'exp_id', 'USI', 'logging_param', '_ml_usecase', 'target_param', 'memory', 'n_jobs_param', 'pipeline', 'fold_groups_param', 'X_test', 'gpu_param', 'gpu_n_jobs_param', 'html_param', 'y_train', 'is_multiclass', 'fold_generator', '_available_plots', 'X_train', 'log_plots_param', 'data', 'X'}
2024-09-04 13:19:10,500:INFO:Checking environment
2024-09-04 13:19:10,501:INFO:python_version: 3.9.19
2024-09-04 13:19:10,503:INFO:python_build: ('main', 'Mar 20 2024 12:50:21')
2024-09-04 13:19:10,503:INFO:machine: x86_64
2024-09-04 13:19:10,505:INFO:platform: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
2024-09-04 13:19:10,506:INFO:Memory: svmem(total=33565290496, available=15600320512, percent=53.5, used=17405034496, free=14351376384, active=1023135744, inactive=16977666048, buffers=46870528, cached=1762009088, shared=88412160, slab=553619456)
2024-09-04 13:19:10,509:INFO:Physical Core: 10
2024-09-04 13:19:10,510:INFO:Logical Core: 20
2024-09-04 13:19:10,511:INFO:Checking libraries
2024-09-04 13:19:10,512:INFO:System:
2024-09-04 13:19:10,513:INFO:    python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21)  [GCC 12.3.0]
2024-09-04 13:19:10,514:INFO:executable: /home/default/miniconda/envs/BMA/bin/python
2024-09-04 13:19:10,515:INFO:   machine: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
2024-09-04 13:19:10,516:INFO:PyCaret required dependencies:
2024-09-04 13:19:10,518:INFO:                 pip: 24.0
2024-09-04 13:19:10,519:INFO:          setuptools: 70.0.0
2024-09-04 13:19:10,520:INFO:             pycaret: 3.3.2
2024-09-04 13:19:10,521:INFO:             IPython: 8.18.1
2024-09-04 13:19:10,523:INFO:          ipywidgets: 8.1.5
2024-09-04 13:19:10,523:INFO:                tqdm: 4.66.4
2024-09-04 13:19:10,525:INFO:               numpy: 1.26.3
2024-09-04 13:19:10,526:INFO:              pandas: 2.1.4
2024-09-04 13:19:10,527:INFO:              jinja2: 3.1.3
2024-09-04 13:19:10,528:INFO:               scipy: 1.11.4
2024-09-04 13:19:10,529:INFO:              joblib: 1.3.2
2024-09-04 13:19:10,530:INFO:             sklearn: 1.4.2
2024-09-04 13:19:10,531:INFO:                pyod: 2.0.1
2024-09-04 13:19:10,532:INFO:            imblearn: 0.12.3
2024-09-04 13:19:10,533:INFO:   category_encoders: 2.6.3
2024-09-04 13:19:10,534:INFO:            lightgbm: 4.5.0
2024-09-04 13:19:10,535:INFO:               numba: 0.60.0
2024-09-04 13:19:10,536:INFO:            requests: 2.32.3
2024-09-04 13:19:10,537:INFO:          matplotlib: 3.7.5
2024-09-04 13:19:10,538:INFO:          scikitplot: 0.3.7
2024-09-04 13:19:10,539:INFO:         yellowbrick: 1.5
2024-09-04 13:19:10,541:INFO:              plotly: 5.22.0
2024-09-04 13:19:10,541:INFO:    plotly-resampler: Not installed
2024-09-04 13:19:10,542:INFO:             kaleido: 0.2.1
2024-09-04 13:19:10,543:INFO:           schemdraw: 0.15
2024-09-04 13:19:10,544:INFO:         statsmodels: 0.14.2
2024-09-04 13:19:10,545:INFO:              sktime: 0.26.0
2024-09-04 13:19:10,546:INFO:               tbats: 1.1.3
2024-09-04 13:19:10,548:INFO:            pmdarima: 2.0.4
2024-09-04 13:19:10,549:INFO:              psutil: 5.9.8
2024-09-04 13:19:10,550:INFO:          markupsafe: 2.1.5
2024-09-04 13:19:10,552:INFO:             pickle5: Not installed
2024-09-04 13:19:10,553:INFO:         cloudpickle: 3.0.0
2024-09-04 13:19:10,554:INFO:         deprecation: 2.1.0
2024-09-04 13:19:10,555:INFO:              xxhash: 3.4.1
2024-09-04 13:19:10,556:INFO:           wurlitzer: 3.1.1
2024-09-04 13:19:10,557:INFO:PyCaret optional dependencies:
2024-09-04 13:19:10,558:INFO:                shap: Not installed
2024-09-04 13:19:10,559:INFO:           interpret: Not installed
2024-09-04 13:19:10,560:INFO:                umap: Not installed
2024-09-04 13:19:10,560:INFO:     ydata_profiling: Not installed
2024-09-04 13:19:10,562:INFO:  explainerdashboard: Not installed
2024-09-04 13:19:10,563:INFO:             autoviz: Not installed
2024-09-04 13:19:10,565:INFO:           fairlearn: Not installed
2024-09-04 13:19:10,566:INFO:          deepchecks: Not installed
2024-09-04 13:19:10,567:INFO:             xgboost: Not installed
2024-09-04 13:19:10,568:INFO:            catboost: Not installed
2024-09-04 13:19:10,569:INFO:              kmodes: Not installed
2024-09-04 13:19:10,570:INFO:             mlxtend: Not installed
2024-09-04 13:19:10,571:INFO:       statsforecast: Not installed
2024-09-04 13:19:10,572:INFO:        tune_sklearn: Not installed
2024-09-04 13:19:10,573:INFO:                 ray: Not installed
2024-09-04 13:19:10,574:INFO:            hyperopt: Not installed
2024-09-04 13:19:10,575:INFO:              optuna: Not installed
2024-09-04 13:19:10,576:INFO:               skopt: Not installed
2024-09-04 13:19:10,577:INFO:              mlflow: 2.14.0
2024-09-04 13:19:10,578:INFO:              gradio: Not installed
2024-09-04 13:19:10,579:INFO:             fastapi: 0.111.0
2024-09-04 13:19:10,579:INFO:             uvicorn: 0.30.1
2024-09-04 13:19:10,581:INFO:              m2cgen: Not installed
2024-09-04 13:19:10,582:INFO:           evidently: Not installed
2024-09-04 13:19:10,583:INFO:               fugue: Not installed
2024-09-04 13:19:10,584:INFO:           streamlit: 1.36.0
2024-09-04 13:19:10,585:INFO:             prophet: Not installed
2024-09-04 13:19:10,586:INFO:None
2024-09-04 13:19:10,587:INFO:Set up GPU usage.
2024-09-04 13:19:10,588:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:10,589:WARNING:cuML is outdated or not found. Required version is >=23.08.
                Please visit https://rapids.ai/install for installation instructions.
2024-09-04 13:19:10,591:INFO:Set up data.
2024-09-04 13:19:11,289:INFO:Set up folding strategy.
2024-09-04 13:19:11,289:INFO:Set up train/test split.
2024-09-04 13:19:11,368:INFO:Set up index.
2024-09-04 13:19:11,374:INFO:Assigning column types.
2024-09-04 13:19:11,521:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-04 13:19:11,523:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:11,555:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-04 13:19:11,556:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:11,556:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:11,557:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 13:19:11,558:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:11,574:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:11,578:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:11,580:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 13:19:11,680:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 13:19:11,681:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:11,754:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-04 13:19:11,755:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:11,756:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:11,757:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 13:19:11,758:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:11,779:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:11,784:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:11,786:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 13:19:11,794:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 13:19:11,798:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-04 13:19:11,799:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:11,911:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:11,912:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:11,913:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 13:19:11,914:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:11,931:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:11,935:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:11,936:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 13:19:11,943:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 13:19:11,949:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:12,012:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:12,013:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:12,014:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 13:19:12,014:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:12,035:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:12,039:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:12,040:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 13:19:12,049:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 13:19:12,053:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-04 13:19:12,055:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:12,126:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:12,127:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:12,128:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:12,144:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:12,147:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:12,148:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 13:19:12,154:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 13:19:12,156:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:12,217:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:12,218:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:12,219:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:12,237:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:12,242:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:12,243:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 13:19:12,253:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 13:19:12,259:INFO:Preparing preprocessing pipeline...
2024-09-04 13:19:12,290:INFO:Set up simple imputation.
2024-09-04 13:19:12,527:INFO:Finished creating preprocessing pipeline.
2024-09-04 13:19:12,532:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-09-04 13:19:12,533:INFO:Creating final display dataframe.
2024-09-04 13:19:13,206:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target         Class_Int
2                   Target type        Multiclass
3           Original data shape       (7716, 385)
4        Transformed data shape       (7716, 385)
5   Transformed train set shape       (5401, 385)
6    Transformed test set shape       (2315, 385)
7              Numeric features               384
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU              True
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              3a5e
2024-09-04 13:19:13,217:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:13,257:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:13,258:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:13,258:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:13,273:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:13,277:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:13,278:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 13:19:13,298:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 13:19:13,299:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:13,372:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:13,373:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:13,374:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:13,396:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:13,400:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 13:19:13,401:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 13:19:13,416:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-04 13:19:13,427:INFO:setup() successfully completed in 2.95s...............
2024-09-04 13:19:27,508:INFO:Initializing compare_models()
2024-09-04 13:19:27,509:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-09-04 13:19:27,509:INFO:Checking exceptions
2024-09-04 13:19:27,563:INFO:Preparing display monitor
2024-09-04 13:19:27,587:INFO:Initializing Logistic Regression
2024-09-04 13:19:27,588:INFO:Total runtime is 1.1873245239257812e-05 minutes
2024-09-04 13:19:27,592:INFO:SubProcess create_model() called ==================================
2024-09-04 13:19:27,593:INFO:Initializing create_model()
2024-09-04 13:19:27,593:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd536546f70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:19:27,594:INFO:Checking exceptions
2024-09-04 13:19:27,594:INFO:Importing libraries
2024-09-04 13:19:27,595:INFO:Copying training dataset
2024-09-04 13:19:27,709:INFO:Defining folds
2024-09-04 13:19:27,709:INFO:Declaring metric variables
2024-09-04 13:19:27,714:INFO:Importing untrained model
2024-09-04 13:19:27,720:INFO:Logistic Regression Imported successfully
2024-09-04 13:19:27,731:INFO:Starting cross validation
2024-09-04 13:19:27,734:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:19:28,683:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:19:29,265:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:19:29,731:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:19:30,374:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:19:30,984:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:19:31,659:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:19:32,341:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:19:33,419:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:19:34,304:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:19:35,252:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:19:35,269:INFO:Calculating mean and std
2024-09-04 13:19:35,273:INFO:Creating metrics dataframe
2024-09-04 13:19:35,280:INFO:Uploading results into container
2024-09-04 13:19:35,281:INFO:Uploading model into container now
2024-09-04 13:19:35,283:INFO:_master_model_container: 1
2024-09-04 13:19:35,284:INFO:_display_container: 2
2024-09-04 13:19:35,285:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-04 13:19:35,288:INFO:create_model() successfully completed......................................
2024-09-04 13:19:35,652:INFO:SubProcess create_model() end ==================================
2024-09-04 13:19:35,652:INFO:Creating metrics dataframe
2024-09-04 13:19:35,660:INFO:Initializing K Neighbors Classifier
2024-09-04 13:19:35,661:INFO:Total runtime is 0.13455713589986165 minutes
2024-09-04 13:19:35,666:INFO:SubProcess create_model() called ==================================
2024-09-04 13:19:35,667:INFO:Initializing create_model()
2024-09-04 13:19:35,668:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd536546f70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:19:35,669:INFO:Checking exceptions
2024-09-04 13:19:35,669:INFO:Importing libraries
2024-09-04 13:19:35,670:INFO:Copying training dataset
2024-09-04 13:19:35,762:INFO:Defining folds
2024-09-04 13:19:35,764:INFO:Declaring metric variables
2024-09-04 13:19:35,772:INFO:Importing untrained model
2024-09-04 13:19:35,780:INFO:K Neighbors Classifier Imported successfully
2024-09-04 13:19:35,790:INFO:Starting cross validation
2024-09-04 13:19:35,792:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:19:35,936:WARNING:huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2024-09-04 13:19:35,939:WARNING:To disable this warning, you can either:
2024-09-04 13:19:35,941:WARNING:	- Avoid using `tokenizers` before the fork if possible
2024-09-04 13:19:35,942:WARNING:	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2024-09-04 13:19:37,129:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:19:37,462:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:19:38,053:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:19:39,032:INFO:Calculating mean and std
2024-09-04 13:19:39,035:INFO:Creating metrics dataframe
2024-09-04 13:19:39,038:INFO:Uploading results into container
2024-09-04 13:19:39,039:INFO:Uploading model into container now
2024-09-04 13:19:39,040:INFO:_master_model_container: 2
2024-09-04 13:19:39,040:INFO:_display_container: 2
2024-09-04 13:19:39,041:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-04 13:19:39,041:INFO:create_model() successfully completed......................................
2024-09-04 13:19:39,460:INFO:SubProcess create_model() end ==================================
2024-09-04 13:19:39,461:INFO:Creating metrics dataframe
2024-09-04 13:19:39,481:INFO:Initializing Naive Bayes
2024-09-04 13:19:39,482:INFO:Total runtime is 0.1982451558113098 minutes
2024-09-04 13:19:39,491:INFO:SubProcess create_model() called ==================================
2024-09-04 13:19:39,493:INFO:Initializing create_model()
2024-09-04 13:19:39,494:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd536546f70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:19:39,495:INFO:Checking exceptions
2024-09-04 13:19:39,497:INFO:Importing libraries
2024-09-04 13:19:39,498:INFO:Copying training dataset
2024-09-04 13:19:39,595:INFO:Defining folds
2024-09-04 13:19:39,602:INFO:Declaring metric variables
2024-09-04 13:19:39,606:INFO:Importing untrained model
2024-09-04 13:19:39,610:INFO:Naive Bayes Imported successfully
2024-09-04 13:19:39,618:INFO:Starting cross validation
2024-09-04 13:19:39,620:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:19:40,634:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:19:40,758:INFO:Calculating mean and std
2024-09-04 13:19:40,760:INFO:Creating metrics dataframe
2024-09-04 13:19:40,763:INFO:Uploading results into container
2024-09-04 13:19:40,764:INFO:Uploading model into container now
2024-09-04 13:19:40,764:INFO:_master_model_container: 3
2024-09-04 13:19:40,765:INFO:_display_container: 2
2024-09-04 13:19:40,765:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-04 13:19:40,766:INFO:create_model() successfully completed......................................
2024-09-04 13:19:41,127:INFO:SubProcess create_model() end ==================================
2024-09-04 13:19:41,127:INFO:Creating metrics dataframe
2024-09-04 13:19:41,135:INFO:Initializing Decision Tree Classifier
2024-09-04 13:19:41,136:INFO:Total runtime is 0.225815745194753 minutes
2024-09-04 13:19:41,140:INFO:SubProcess create_model() called ==================================
2024-09-04 13:19:41,141:INFO:Initializing create_model()
2024-09-04 13:19:41,142:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd536546f70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:19:41,143:INFO:Checking exceptions
2024-09-04 13:19:41,143:INFO:Importing libraries
2024-09-04 13:19:41,144:INFO:Copying training dataset
2024-09-04 13:19:41,212:INFO:Defining folds
2024-09-04 13:19:41,213:INFO:Declaring metric variables
2024-09-04 13:19:41,218:INFO:Importing untrained model
2024-09-04 13:19:41,222:INFO:Decision Tree Classifier Imported successfully
2024-09-04 13:19:41,228:INFO:Starting cross validation
2024-09-04 13:19:41,230:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:20:06,740:INFO:Calculating mean and std
2024-09-04 13:20:06,742:INFO:Creating metrics dataframe
2024-09-04 13:20:06,746:INFO:Uploading results into container
2024-09-04 13:20:06,747:INFO:Uploading model into container now
2024-09-04 13:20:06,749:INFO:_master_model_container: 4
2024-09-04 13:20:06,750:INFO:_display_container: 2
2024-09-04 13:20:06,751:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-04 13:20:06,752:INFO:create_model() successfully completed......................................
2024-09-04 13:20:07,185:INFO:SubProcess create_model() end ==================================
2024-09-04 13:20:07,186:INFO:Creating metrics dataframe
2024-09-04 13:20:07,199:INFO:Initializing SVM - Linear Kernel
2024-09-04 13:20:07,200:INFO:Total runtime is 0.6602063377698262 minutes
2024-09-04 13:20:07,209:INFO:SubProcess create_model() called ==================================
2024-09-04 13:20:07,210:INFO:Initializing create_model()
2024-09-04 13:20:07,211:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd536546f70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:20:07,212:INFO:Checking exceptions
2024-09-04 13:20:07,213:INFO:Importing libraries
2024-09-04 13:20:07,214:INFO:Copying training dataset
2024-09-04 13:20:07,315:INFO:Defining folds
2024-09-04 13:20:07,316:INFO:Declaring metric variables
2024-09-04 13:20:07,322:INFO:Importing untrained model
2024-09-04 13:20:07,330:INFO:SVM - Linear Kernel Imported successfully
2024-09-04 13:20:07,344:INFO:Starting cross validation
2024-09-04 13:20:07,347:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:20:07,615:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:07,808:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:08,020:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:08,223:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:08,422:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:08,660:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:08,881:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:09,097:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:09,310:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:09,505:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:09,522:INFO:Calculating mean and std
2024-09-04 13:20:09,525:INFO:Creating metrics dataframe
2024-09-04 13:20:09,530:INFO:Uploading results into container
2024-09-04 13:20:09,532:INFO:Uploading model into container now
2024-09-04 13:20:09,533:INFO:_master_model_container: 5
2024-09-04 13:20:09,534:INFO:_display_container: 2
2024-09-04 13:20:09,535:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-04 13:20:09,535:INFO:create_model() successfully completed......................................
2024-09-04 13:20:09,906:INFO:SubProcess create_model() end ==================================
2024-09-04 13:20:09,908:INFO:Creating metrics dataframe
2024-09-04 13:20:09,929:INFO:Initializing Ridge Classifier
2024-09-04 13:20:09,930:INFO:Total runtime is 0.7057187954584757 minutes
2024-09-04 13:20:09,940:INFO:SubProcess create_model() called ==================================
2024-09-04 13:20:09,941:INFO:Initializing create_model()
2024-09-04 13:20:09,942:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd536546f70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:20:09,943:INFO:Checking exceptions
2024-09-04 13:20:09,943:INFO:Importing libraries
2024-09-04 13:20:09,945:INFO:Copying training dataset
2024-09-04 13:20:10,009:INFO:Defining folds
2024-09-04 13:20:10,010:INFO:Declaring metric variables
2024-09-04 13:20:10,015:INFO:Importing untrained model
2024-09-04 13:20:10,019:INFO:Ridge Classifier Imported successfully
2024-09-04 13:20:10,025:INFO:Starting cross validation
2024-09-04 13:20:10,027:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:20:10,188:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:10,363:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:10,545:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:10,555:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:10,746:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:10,948:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:10,955:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:11,090:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:11,095:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:11,269:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:11,276:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:11,432:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:11,617:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:11,623:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:11,769:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:11,774:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:11,785:INFO:Calculating mean and std
2024-09-04 13:20:11,790:INFO:Creating metrics dataframe
2024-09-04 13:20:11,794:INFO:Uploading results into container
2024-09-04 13:20:11,795:INFO:Uploading model into container now
2024-09-04 13:20:11,796:INFO:_master_model_container: 6
2024-09-04 13:20:11,796:INFO:_display_container: 2
2024-09-04 13:20:11,801:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-04 13:20:11,803:INFO:create_model() successfully completed......................................
2024-09-04 13:20:12,170:INFO:SubProcess create_model() end ==================================
2024-09-04 13:20:12,171:INFO:Creating metrics dataframe
2024-09-04 13:20:12,182:INFO:Initializing Random Forest Classifier
2024-09-04 13:20:12,182:INFO:Total runtime is 0.7432552099227905 minutes
2024-09-04 13:20:12,192:INFO:SubProcess create_model() called ==================================
2024-09-04 13:20:12,193:INFO:Initializing create_model()
2024-09-04 13:20:12,193:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd536546f70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:20:12,194:INFO:Checking exceptions
2024-09-04 13:20:12,196:INFO:Importing libraries
2024-09-04 13:20:12,198:INFO:Copying training dataset
2024-09-04 13:20:12,268:INFO:Defining folds
2024-09-04 13:20:12,269:INFO:Declaring metric variables
2024-09-04 13:20:12,273:INFO:Importing untrained model
2024-09-04 13:20:12,277:INFO:Random Forest Classifier Imported successfully
2024-09-04 13:20:12,283:INFO:Starting cross validation
2024-09-04 13:20:12,285:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:20:16,125:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:20,803:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:21,743:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:21,749:INFO:Calculating mean and std
2024-09-04 13:20:21,750:INFO:Creating metrics dataframe
2024-09-04 13:20:21,752:INFO:Uploading results into container
2024-09-04 13:20:21,753:INFO:Uploading model into container now
2024-09-04 13:20:21,754:INFO:_master_model_container: 7
2024-09-04 13:20:21,755:INFO:_display_container: 2
2024-09-04 13:20:21,755:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-04 13:20:21,756:INFO:create_model() successfully completed......................................
2024-09-04 13:20:22,142:INFO:SubProcess create_model() end ==================================
2024-09-04 13:20:22,143:INFO:Creating metrics dataframe
2024-09-04 13:20:22,153:INFO:Initializing Quadratic Discriminant Analysis
2024-09-04 13:20:22,153:INFO:Total runtime is 0.9094386974970499 minutes
2024-09-04 13:20:22,158:INFO:SubProcess create_model() called ==================================
2024-09-04 13:20:22,159:INFO:Initializing create_model()
2024-09-04 13:20:22,160:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd536546f70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:20:22,160:INFO:Checking exceptions
2024-09-04 13:20:22,161:INFO:Importing libraries
2024-09-04 13:20:22,161:INFO:Copying training dataset
2024-09-04 13:20:22,221:INFO:Defining folds
2024-09-04 13:20:22,222:INFO:Declaring metric variables
2024-09-04 13:20:22,226:INFO:Importing untrained model
2024-09-04 13:20:22,230:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-04 13:20:22,237:INFO:Starting cross validation
2024-09-04 13:20:22,238:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:20:22,619:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 13:20:23,008:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:23,017:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:23,306:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 13:20:23,976:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:23,986:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:24,312:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 13:20:24,586:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:24,601:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:24,823:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 13:20:25,194:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:25,203:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:25,631:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 13:20:25,941:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:25,955:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:26,971:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 13:20:28,899:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:28,909:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:29,246:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 13:20:29,570:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:29,579:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:29,952:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 13:20:30,296:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:30,307:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:30,562:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 13:20:30,841:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:30,851:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:31,247:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 13:20:31,775:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:31,784:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:31,798:INFO:Calculating mean and std
2024-09-04 13:20:31,801:INFO:Creating metrics dataframe
2024-09-04 13:20:31,805:INFO:Uploading results into container
2024-09-04 13:20:31,811:INFO:Uploading model into container now
2024-09-04 13:20:31,813:INFO:_master_model_container: 8
2024-09-04 13:20:31,813:INFO:_display_container: 2
2024-09-04 13:20:31,814:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-04 13:20:31,814:INFO:create_model() successfully completed......................................
2024-09-04 13:20:32,164:INFO:SubProcess create_model() end ==================================
2024-09-04 13:20:32,166:INFO:Creating metrics dataframe
2024-09-04 13:20:32,188:INFO:Initializing Ada Boost Classifier
2024-09-04 13:20:32,190:INFO:Total runtime is 1.0767024079958596 minutes
2024-09-04 13:20:32,204:INFO:SubProcess create_model() called ==================================
2024-09-04 13:20:32,206:INFO:Initializing create_model()
2024-09-04 13:20:32,207:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd536546f70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:20:32,208:INFO:Checking exceptions
2024-09-04 13:20:32,209:INFO:Importing libraries
2024-09-04 13:20:32,210:INFO:Copying training dataset
2024-09-04 13:20:32,282:INFO:Defining folds
2024-09-04 13:20:32,283:INFO:Declaring metric variables
2024-09-04 13:20:32,288:INFO:Importing untrained model
2024-09-04 13:20:32,293:INFO:Ada Boost Classifier Imported successfully
2024-09-04 13:20:32,300:INFO:Starting cross validation
2024-09-04 13:20:32,303:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:20:32,367:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 13:20:45,539:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:45,546:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:45,607:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 13:20:58,591:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:20:58,597:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:20:58,655:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 13:21:12,215:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:21:12,220:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:21:12,273:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 13:21:25,627:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:21:25,631:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:21:25,684:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 13:21:38,514:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:21:38,574:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 13:21:51,611:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:21:51,617:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:21:51,687:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 13:22:04,231:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:22:04,235:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:22:04,291:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 13:22:17,407:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:22:17,517:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 13:22:30,149:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:22:30,155:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:22:30,213:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 13:22:43,187:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:22:43,191:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:22:43,196:INFO:Calculating mean and std
2024-09-04 13:22:43,198:INFO:Creating metrics dataframe
2024-09-04 13:22:43,200:INFO:Uploading results into container
2024-09-04 13:22:43,201:INFO:Uploading model into container now
2024-09-04 13:22:43,202:INFO:_master_model_container: 9
2024-09-04 13:22:43,202:INFO:_display_container: 2
2024-09-04 13:22:43,203:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-04 13:22:43,204:INFO:create_model() successfully completed......................................
2024-09-04 13:22:43,614:INFO:SubProcess create_model() end ==================================
2024-09-04 13:22:43,614:INFO:Creating metrics dataframe
2024-09-04 13:22:43,624:INFO:Initializing Gradient Boosting Classifier
2024-09-04 13:22:43,624:INFO:Total runtime is 3.267288398742676 minutes
2024-09-04 13:22:43,628:INFO:SubProcess create_model() called ==================================
2024-09-04 13:22:43,629:INFO:Initializing create_model()
2024-09-04 13:22:43,638:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd536546f70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:22:43,639:INFO:Checking exceptions
2024-09-04 13:22:43,639:INFO:Importing libraries
2024-09-04 13:22:43,639:INFO:Copying training dataset
2024-09-04 13:22:43,711:INFO:Defining folds
2024-09-04 13:22:43,712:INFO:Declaring metric variables
2024-09-04 13:22:43,715:INFO:Importing untrained model
2024-09-04 13:22:43,720:INFO:Gradient Boosting Classifier Imported successfully
2024-09-04 13:22:43,727:INFO:Starting cross validation
2024-09-04 13:22:43,728:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:33:44,516:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:38:20,019:INFO:Initializing compare_models()
2024-09-04 13:38:20,020:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, 'include': None, 'exclude': ['lightgbm', 'gbc', 'xgboost'], 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=['lightgbm', 'gbc', 'xgboost'])
2024-09-04 13:38:20,021:INFO:Checking exceptions
2024-09-04 13:38:20,058:INFO:Preparing display monitor
2024-09-04 13:38:20,082:INFO:Initializing Logistic Regression
2024-09-04 13:38:20,083:INFO:Total runtime is 1.2004375457763672e-05 minutes
2024-09-04 13:38:20,089:INFO:SubProcess create_model() called ==================================
2024-09-04 13:38:20,090:INFO:Initializing create_model()
2024-09-04 13:38:20,091:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd6b8d6fb80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:38:20,091:INFO:Checking exceptions
2024-09-04 13:38:20,092:INFO:Importing libraries
2024-09-04 13:38:20,092:INFO:Copying training dataset
2024-09-04 13:38:20,206:INFO:Defining folds
2024-09-04 13:38:20,207:INFO:Declaring metric variables
2024-09-04 13:38:20,214:INFO:Importing untrained model
2024-09-04 13:38:20,218:INFO:Logistic Regression Imported successfully
2024-09-04 13:38:20,225:INFO:Starting cross validation
2024-09-04 13:38:20,227:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:38:20,837:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:38:21,238:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:38:21,822:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:38:22,353:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:38:22,997:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:38:24,133:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:38:25,859:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:38:26,747:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:38:28,475:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:38:33,012:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:38:33,062:INFO:Calculating mean and std
2024-09-04 13:38:33,091:INFO:Creating metrics dataframe
2024-09-04 13:38:33,098:INFO:Uploading results into container
2024-09-04 13:38:33,100:INFO:Uploading model into container now
2024-09-04 13:38:33,102:INFO:_master_model_container: 10
2024-09-04 13:38:33,102:INFO:_display_container: 2
2024-09-04 13:38:33,103:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-04 13:38:33,104:INFO:create_model() successfully completed......................................
2024-09-04 13:38:33,674:INFO:SubProcess create_model() end ==================================
2024-09-04 13:38:33,675:INFO:Creating metrics dataframe
2024-09-04 13:38:33,693:INFO:Initializing K Neighbors Classifier
2024-09-04 13:38:33,694:INFO:Total runtime is 0.2268738826115926 minutes
2024-09-04 13:38:33,705:INFO:SubProcess create_model() called ==================================
2024-09-04 13:38:33,706:INFO:Initializing create_model()
2024-09-04 13:38:33,707:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd6b8d6fb80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:38:33,707:INFO:Checking exceptions
2024-09-04 13:38:33,708:INFO:Importing libraries
2024-09-04 13:38:33,708:INFO:Copying training dataset
2024-09-04 13:38:33,947:INFO:Defining folds
2024-09-04 13:38:33,949:INFO:Declaring metric variables
2024-09-04 13:38:33,964:INFO:Importing untrained model
2024-09-04 13:38:33,984:INFO:K Neighbors Classifier Imported successfully
2024-09-04 13:38:34,022:INFO:Starting cross validation
2024-09-04 13:38:34,028:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:38:35,208:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:38:35,470:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:38:35,906:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:38:36,716:INFO:Calculating mean and std
2024-09-04 13:38:36,718:INFO:Creating metrics dataframe
2024-09-04 13:38:36,721:INFO:Uploading results into container
2024-09-04 13:38:36,723:INFO:Uploading model into container now
2024-09-04 13:38:36,724:INFO:_master_model_container: 11
2024-09-04 13:38:36,726:INFO:_display_container: 2
2024-09-04 13:38:36,726:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-04 13:38:36,727:INFO:create_model() successfully completed......................................
2024-09-04 13:38:37,192:INFO:SubProcess create_model() end ==================================
2024-09-04 13:38:37,193:INFO:Creating metrics dataframe
2024-09-04 13:38:37,202:INFO:Initializing Naive Bayes
2024-09-04 13:38:37,203:INFO:Total runtime is 0.28535297711690266 minutes
2024-09-04 13:38:37,215:INFO:SubProcess create_model() called ==================================
2024-09-04 13:38:37,217:INFO:Initializing create_model()
2024-09-04 13:38:37,218:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd6b8d6fb80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:38:37,220:INFO:Checking exceptions
2024-09-04 13:38:37,221:INFO:Importing libraries
2024-09-04 13:38:37,222:INFO:Copying training dataset
2024-09-04 13:38:37,311:INFO:Defining folds
2024-09-04 13:38:37,312:INFO:Declaring metric variables
2024-09-04 13:38:37,317:INFO:Importing untrained model
2024-09-04 13:38:37,321:INFO:Naive Bayes Imported successfully
2024-09-04 13:38:37,329:INFO:Starting cross validation
2024-09-04 13:38:37,331:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:38:38,696:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:38:38,817:INFO:Calculating mean and std
2024-09-04 13:38:38,819:INFO:Creating metrics dataframe
2024-09-04 13:38:38,823:INFO:Uploading results into container
2024-09-04 13:38:38,824:INFO:Uploading model into container now
2024-09-04 13:38:38,826:INFO:_master_model_container: 12
2024-09-04 13:38:38,826:INFO:_display_container: 2
2024-09-04 13:38:38,828:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-04 13:38:38,829:INFO:create_model() successfully completed......................................
2024-09-04 13:38:39,280:INFO:SubProcess create_model() end ==================================
2024-09-04 13:38:39,283:INFO:Creating metrics dataframe
2024-09-04 13:38:39,299:INFO:Initializing Decision Tree Classifier
2024-09-04 13:38:39,301:INFO:Total runtime is 0.32031975189844764 minutes
2024-09-04 13:38:39,309:INFO:SubProcess create_model() called ==================================
2024-09-04 13:38:39,310:INFO:Initializing create_model()
2024-09-04 13:38:39,311:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd6b8d6fb80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:38:39,311:INFO:Checking exceptions
2024-09-04 13:38:39,312:INFO:Importing libraries
2024-09-04 13:38:39,315:INFO:Copying training dataset
2024-09-04 13:38:39,460:INFO:Defining folds
2024-09-04 13:38:39,461:INFO:Declaring metric variables
2024-09-04 13:38:39,466:INFO:Importing untrained model
2024-09-04 13:38:39,480:INFO:Decision Tree Classifier Imported successfully
2024-09-04 13:38:39,504:INFO:Starting cross validation
2024-09-04 13:38:39,511:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:39:06,902:INFO:Calculating mean and std
2024-09-04 13:39:06,904:INFO:Creating metrics dataframe
2024-09-04 13:39:06,907:INFO:Uploading results into container
2024-09-04 13:39:06,908:INFO:Uploading model into container now
2024-09-04 13:39:06,909:INFO:_master_model_container: 13
2024-09-04 13:39:06,910:INFO:_display_container: 2
2024-09-04 13:39:06,911:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-04 13:39:06,911:INFO:create_model() successfully completed......................................
2024-09-04 13:39:07,301:INFO:SubProcess create_model() end ==================================
2024-09-04 13:39:07,302:INFO:Creating metrics dataframe
2024-09-04 13:39:07,314:INFO:Initializing SVM - Linear Kernel
2024-09-04 13:39:07,315:INFO:Total runtime is 0.7872110803922017 minutes
2024-09-04 13:39:07,324:INFO:SubProcess create_model() called ==================================
2024-09-04 13:39:07,325:INFO:Initializing create_model()
2024-09-04 13:39:07,325:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd6b8d6fb80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:39:07,326:INFO:Checking exceptions
2024-09-04 13:39:07,327:INFO:Importing libraries
2024-09-04 13:39:07,327:INFO:Copying training dataset
2024-09-04 13:39:07,398:INFO:Defining folds
2024-09-04 13:39:07,399:INFO:Declaring metric variables
2024-09-04 13:39:07,403:INFO:Importing untrained model
2024-09-04 13:39:07,408:INFO:SVM - Linear Kernel Imported successfully
2024-09-04 13:39:07,416:INFO:Starting cross validation
2024-09-04 13:39:07,418:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:39:07,643:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:07,898:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:08,095:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:08,315:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:08,536:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:08,753:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:09,000:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:09,280:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:09,492:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:09,812:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:09,829:INFO:Calculating mean and std
2024-09-04 13:39:09,832:INFO:Creating metrics dataframe
2024-09-04 13:39:09,835:INFO:Uploading results into container
2024-09-04 13:39:09,838:INFO:Uploading model into container now
2024-09-04 13:39:09,844:INFO:_master_model_container: 14
2024-09-04 13:39:09,845:INFO:_display_container: 2
2024-09-04 13:39:09,846:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-04 13:39:09,847:INFO:create_model() successfully completed......................................
2024-09-04 13:39:10,165:INFO:SubProcess create_model() end ==================================
2024-09-04 13:39:10,166:INFO:Creating metrics dataframe
2024-09-04 13:39:10,174:INFO:Initializing Ridge Classifier
2024-09-04 13:39:10,175:INFO:Total runtime is 0.834879966576894 minutes
2024-09-04 13:39:10,180:INFO:SubProcess create_model() called ==================================
2024-09-04 13:39:10,182:INFO:Initializing create_model()
2024-09-04 13:39:10,182:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd6b8d6fb80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:39:10,182:INFO:Checking exceptions
2024-09-04 13:39:10,183:INFO:Importing libraries
2024-09-04 13:39:10,184:INFO:Copying training dataset
2024-09-04 13:39:10,249:INFO:Defining folds
2024-09-04 13:39:10,250:INFO:Declaring metric variables
2024-09-04 13:39:10,254:INFO:Importing untrained model
2024-09-04 13:39:10,258:INFO:Ridge Classifier Imported successfully
2024-09-04 13:39:10,265:INFO:Starting cross validation
2024-09-04 13:39:10,266:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:39:10,441:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:10,622:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:10,803:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:10,811:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:10,991:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:11,147:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:11,152:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:11,323:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:11,330:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:11,508:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:11,518:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:11,695:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:11,828:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:11,837:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:11,981:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:11,986:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:11,996:INFO:Calculating mean and std
2024-09-04 13:39:11,998:INFO:Creating metrics dataframe
2024-09-04 13:39:12,001:INFO:Uploading results into container
2024-09-04 13:39:12,006:INFO:Uploading model into container now
2024-09-04 13:39:12,007:INFO:_master_model_container: 15
2024-09-04 13:39:12,007:INFO:_display_container: 2
2024-09-04 13:39:12,008:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-04 13:39:12,008:INFO:create_model() successfully completed......................................
2024-09-04 13:39:12,407:INFO:SubProcess create_model() end ==================================
2024-09-04 13:39:12,409:INFO:Creating metrics dataframe
2024-09-04 13:39:12,427:INFO:Initializing Random Forest Classifier
2024-09-04 13:39:12,428:INFO:Total runtime is 0.8724265416463215 minutes
2024-09-04 13:39:12,434:INFO:SubProcess create_model() called ==================================
2024-09-04 13:39:12,436:INFO:Initializing create_model()
2024-09-04 13:39:12,437:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd6b8d6fb80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:39:12,438:INFO:Checking exceptions
2024-09-04 13:39:12,439:INFO:Importing libraries
2024-09-04 13:39:12,440:INFO:Copying training dataset
2024-09-04 13:39:12,539:INFO:Defining folds
2024-09-04 13:39:12,540:INFO:Declaring metric variables
2024-09-04 13:39:12,544:INFO:Importing untrained model
2024-09-04 13:39:12,548:INFO:Random Forest Classifier Imported successfully
2024-09-04 13:39:12,556:INFO:Starting cross validation
2024-09-04 13:39:12,558:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:39:16,288:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:21,113:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:22,132:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:22,139:INFO:Calculating mean and std
2024-09-04 13:39:22,141:INFO:Creating metrics dataframe
2024-09-04 13:39:22,144:INFO:Uploading results into container
2024-09-04 13:39:22,145:INFO:Uploading model into container now
2024-09-04 13:39:22,146:INFO:_master_model_container: 16
2024-09-04 13:39:22,147:INFO:_display_container: 2
2024-09-04 13:39:22,147:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-04 13:39:22,148:INFO:create_model() successfully completed......................................
2024-09-04 13:39:22,630:INFO:SubProcess create_model() end ==================================
2024-09-04 13:39:22,631:INFO:Creating metrics dataframe
2024-09-04 13:39:22,639:INFO:Initializing Quadratic Discriminant Analysis
2024-09-04 13:39:22,640:INFO:Total runtime is 1.0426352818806965 minutes
2024-09-04 13:39:22,644:INFO:SubProcess create_model() called ==================================
2024-09-04 13:39:22,644:INFO:Initializing create_model()
2024-09-04 13:39:22,645:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd6b8d6fb80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:39:22,645:INFO:Checking exceptions
2024-09-04 13:39:22,646:INFO:Importing libraries
2024-09-04 13:39:22,646:INFO:Copying training dataset
2024-09-04 13:39:22,714:INFO:Defining folds
2024-09-04 13:39:22,715:INFO:Declaring metric variables
2024-09-04 13:39:22,720:INFO:Importing untrained model
2024-09-04 13:39:22,725:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-04 13:39:22,731:INFO:Starting cross validation
2024-09-04 13:39:22,733:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:39:23,010:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 13:39:23,293:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:23,301:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:23,529:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 13:39:23,874:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:23,883:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:24,159:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 13:39:24,373:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:24,382:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:24,701:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 13:39:25,098:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:25,106:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:25,383:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 13:39:25,725:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:25,736:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:25,963:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 13:39:26,263:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:26,273:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:26,614:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 13:39:26,876:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:26,886:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:27,281:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 13:39:27,678:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:27,688:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:27,956:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 13:39:28,324:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:28,332:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:28,688:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-04 13:39:28,930:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:28,939:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:28,951:INFO:Calculating mean and std
2024-09-04 13:39:28,962:INFO:Creating metrics dataframe
2024-09-04 13:39:28,967:INFO:Uploading results into container
2024-09-04 13:39:28,970:INFO:Uploading model into container now
2024-09-04 13:39:28,973:INFO:_master_model_container: 17
2024-09-04 13:39:28,974:INFO:_display_container: 2
2024-09-04 13:39:28,977:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-04 13:39:28,977:INFO:create_model() successfully completed......................................
2024-09-04 13:39:29,359:INFO:SubProcess create_model() end ==================================
2024-09-04 13:39:29,359:INFO:Creating metrics dataframe
2024-09-04 13:39:29,368:INFO:Initializing Ada Boost Classifier
2024-09-04 13:39:29,368:INFO:Total runtime is 1.1547715067863464 minutes
2024-09-04 13:39:29,372:INFO:SubProcess create_model() called ==================================
2024-09-04 13:39:29,372:INFO:Initializing create_model()
2024-09-04 13:39:29,373:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd6b8d6fb80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:39:29,373:INFO:Checking exceptions
2024-09-04 13:39:29,373:INFO:Importing libraries
2024-09-04 13:39:29,374:INFO:Copying training dataset
2024-09-04 13:39:29,447:INFO:Defining folds
2024-09-04 13:39:29,448:INFO:Declaring metric variables
2024-09-04 13:39:29,453:INFO:Importing untrained model
2024-09-04 13:39:29,457:INFO:Ada Boost Classifier Imported successfully
2024-09-04 13:39:29,466:INFO:Starting cross validation
2024-09-04 13:39:29,468:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:39:29,546:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 13:39:42,437:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:42,444:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:42,504:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 13:39:54,638:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:39:54,642:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:39:54,708:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 13:40:07,845:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:40:07,853:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:40:07,936:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 13:40:20,832:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:40:20,839:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:40:20,938:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 13:40:34,113:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:40:34,224:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 13:40:47,587:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:40:47,591:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:40:47,652:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 13:41:00,656:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:41:00,662:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:41:00,726:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 13:41:14,209:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:41:14,294:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 13:41:27,670:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:41:27,674:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:41:27,749:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 13:41:41,220:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:41:41,224:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:41:41,230:INFO:Calculating mean and std
2024-09-04 13:41:41,232:INFO:Creating metrics dataframe
2024-09-04 13:41:41,235:INFO:Uploading results into container
2024-09-04 13:41:41,236:INFO:Uploading model into container now
2024-09-04 13:41:41,237:INFO:_master_model_container: 18
2024-09-04 13:41:41,237:INFO:_display_container: 2
2024-09-04 13:41:41,239:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-04 13:41:41,240:INFO:create_model() successfully completed......................................
2024-09-04 13:41:41,661:INFO:SubProcess create_model() end ==================================
2024-09-04 13:41:41,662:INFO:Creating metrics dataframe
2024-09-04 13:41:41,673:INFO:Initializing Linear Discriminant Analysis
2024-09-04 13:41:41,674:INFO:Total runtime is 3.3598730683326723 minutes
2024-09-04 13:41:41,682:INFO:SubProcess create_model() called ==================================
2024-09-04 13:41:41,684:INFO:Initializing create_model()
2024-09-04 13:41:41,687:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd6b8d6fb80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:41:41,689:INFO:Checking exceptions
2024-09-04 13:41:41,690:INFO:Importing libraries
2024-09-04 13:41:41,691:INFO:Copying training dataset
2024-09-04 13:41:41,765:INFO:Defining folds
2024-09-04 13:41:41,766:INFO:Declaring metric variables
2024-09-04 13:41:41,770:INFO:Importing untrained model
2024-09-04 13:41:41,775:INFO:Linear Discriminant Analysis Imported successfully
2024-09-04 13:41:41,782:INFO:Starting cross validation
2024-09-04 13:41:41,784:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:41:42,124:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:41:42,809:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:41:43,135:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:41:43,462:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:41:43,878:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:41:44,182:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:41:44,486:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:41:44,958:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:41:45,387:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:41:45,844:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-04 13:41:45,873:INFO:Calculating mean and std
2024-09-04 13:41:45,878:INFO:Creating metrics dataframe
2024-09-04 13:41:45,884:INFO:Uploading results into container
2024-09-04 13:41:45,890:INFO:Uploading model into container now
2024-09-04 13:41:45,892:INFO:_master_model_container: 19
2024-09-04 13:41:45,893:INFO:_display_container: 2
2024-09-04 13:41:45,894:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-04 13:41:45,894:INFO:create_model() successfully completed......................................
2024-09-04 13:41:46,334:INFO:SubProcess create_model() end ==================================
2024-09-04 13:41:46,336:INFO:Creating metrics dataframe
2024-09-04 13:41:46,359:INFO:Initializing Extra Trees Classifier
2024-09-04 13:41:46,360:INFO:Total runtime is 3.4379744648933412 minutes
2024-09-04 13:41:46,366:INFO:SubProcess create_model() called ==================================
2024-09-04 13:41:46,367:INFO:Initializing create_model()
2024-09-04 13:41:46,367:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd6b8d6fb80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:41:46,367:INFO:Checking exceptions
2024-09-04 13:41:46,369:INFO:Importing libraries
2024-09-04 13:41:46,369:INFO:Copying training dataset
2024-09-04 13:41:46,433:INFO:Defining folds
2024-09-04 13:41:46,434:INFO:Declaring metric variables
2024-09-04 13:41:46,439:INFO:Importing untrained model
2024-09-04 13:41:46,443:INFO:Extra Trees Classifier Imported successfully
2024-09-04 13:41:46,450:INFO:Starting cross validation
2024-09-04 13:41:46,452:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:41:50,442:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:41:50,899:INFO:Calculating mean and std
2024-09-04 13:41:50,901:INFO:Creating metrics dataframe
2024-09-04 13:41:50,904:INFO:Uploading results into container
2024-09-04 13:41:50,905:INFO:Uploading model into container now
2024-09-04 13:41:50,905:INFO:_master_model_container: 20
2024-09-04 13:41:50,906:INFO:_display_container: 2
2024-09-04 13:41:50,907:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-04 13:41:50,907:INFO:create_model() successfully completed......................................
2024-09-04 13:41:51,334:INFO:SubProcess create_model() end ==================================
2024-09-04 13:41:51,335:INFO:Creating metrics dataframe
2024-09-04 13:41:51,360:INFO:Initializing Dummy Classifier
2024-09-04 13:41:51,361:INFO:Total runtime is 3.521318558851878 minutes
2024-09-04 13:41:51,371:INFO:SubProcess create_model() called ==================================
2024-09-04 13:41:51,373:INFO:Initializing create_model()
2024-09-04 13:41:51,374:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd6b8d6fb80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:41:51,375:INFO:Checking exceptions
2024-09-04 13:41:51,375:INFO:Importing libraries
2024-09-04 13:41:51,376:INFO:Copying training dataset
2024-09-04 13:41:51,471:INFO:Defining folds
2024-09-04 13:41:51,472:INFO:Declaring metric variables
2024-09-04 13:41:51,477:INFO:Importing untrained model
2024-09-04 13:41:51,483:INFO:Dummy Classifier Imported successfully
2024-09-04 13:41:51,494:INFO:Starting cross validation
2024-09-04 13:41:51,496:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2024-09-04 13:41:51,568:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:41:51,652:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:41:51,728:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:41:51,804:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:41:51,905:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:41:51,993:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:41:52,072:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:41:52,169:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:41:52,290:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:41:52,368:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 13:41:52,373:INFO:Calculating mean and std
2024-09-04 13:41:52,378:INFO:Creating metrics dataframe
2024-09-04 13:41:52,380:INFO:Uploading results into container
2024-09-04 13:41:52,384:INFO:Uploading model into container now
2024-09-04 13:41:52,385:INFO:_master_model_container: 21
2024-09-04 13:41:52,385:INFO:_display_container: 2
2024-09-04 13:41:52,386:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-04 13:41:52,387:INFO:create_model() successfully completed......................................
2024-09-04 13:41:52,743:INFO:SubProcess create_model() end ==================================
2024-09-04 13:41:52,744:INFO:Creating metrics dataframe
2024-09-04 13:41:52,756:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2024-09-04 13:41:52,775:INFO:Initializing create_model()
2024-09-04 13:41:52,776:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 13:41:52,777:INFO:Checking exceptions
2024-09-04 13:41:52,784:INFO:Importing libraries
2024-09-04 13:41:52,793:INFO:Copying training dataset
2024-09-04 13:41:52,857:INFO:Defining folds
2024-09-04 13:41:52,858:INFO:Declaring metric variables
2024-09-04 13:41:52,859:INFO:Importing untrained model
2024-09-04 13:41:52,860:INFO:Declaring custom model
2024-09-04 13:41:52,861:INFO:SVM - Linear Kernel Imported successfully
2024-09-04 13:41:52,864:INFO:Cross validation set to False
2024-09-04 13:41:52,864:INFO:Fitting Model
2024-09-04 13:41:53,081:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-04 13:41:53,082:INFO:create_model() successfully completed......................................
2024-09-04 13:41:53,540:INFO:_master_model_container: 21
2024-09-04 13:41:53,543:INFO:_display_container: 2
2024-09-04 13:41:53,545:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-04 13:41:53,547:INFO:compare_models() successfully completed......................................
2024-09-04 13:42:29,880:INFO:Initializing plot_model()
2024-09-04 13:42:29,880:INFO:plot_model(plot=confusion_matrix, fold=None, verbose=True, display=None, display_format=None, estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fd5363358e0>, system=True)
2024-09-04 13:42:29,881:INFO:Checking exceptions
2024-09-04 13:42:29,916:INFO:Preloading libraries
2024-09-04 13:42:29,917:INFO:Copying training dataset
2024-09-04 13:42:29,918:INFO:Plot type: confusion_matrix
2024-09-04 13:42:30,661:INFO:Fitting Model
2024-09-04 13:42:30,662:WARNING:/home/default/miniconda/envs/BMA/lib/python3.9/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names
  warnings.warn(

2024-09-04 13:42:30,663:INFO:Scoring test/hold-out set
2024-09-04 13:42:30,781:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,789:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,791:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,791:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,792:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,793:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,794:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,795:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,796:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,796:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,797:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,801:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,802:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,806:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,810:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,811:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,814:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,815:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,817:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,818:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,819:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,820:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,821:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,823:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,824:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,826:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,827:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,828:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,928:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,929:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,930:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,931:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,932:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,933:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,934:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,935:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,936:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,936:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,937:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,939:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,940:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,940:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,941:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,945:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,946:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,948:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,949:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,950:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,951:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,953:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,954:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,955:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,956:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,958:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,959:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,960:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif

2024-09-04 13:42:30,963:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,964:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,965:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,966:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,967:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,968:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,969:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,970:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,971:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,971:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,974:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,977:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,978:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,978:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,979:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,980:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,981:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,981:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,982:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,983:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,983:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,984:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,984:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,985:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,986:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,987:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,987:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,988:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,989:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,990:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,990:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,991:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,992:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,993:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,994:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,995:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,996:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,996:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,997:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,997:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,998:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,998:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:30,999:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,000:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,000:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,001:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,002:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,003:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,003:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,004:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,004:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,005:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,006:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,007:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,007:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,008:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,008:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,010:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,010:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,011:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,012:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,013:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,014:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,014:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,015:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,016:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,016:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,017:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,018:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,019:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,019:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,020:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,021:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,021:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,022:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,023:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,024:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,024:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,025:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,026:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,027:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,028:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,029:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,030:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,030:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,031:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,032:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,033:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,033:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,034:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,035:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,036:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,037:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,038:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,038:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,039:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,040:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,041:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,042:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,042:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,043:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,044:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,045:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,047:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,049:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,050:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,050:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,051:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,052:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,054:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,056:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,057:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,058:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,059:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,060:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,061:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,063:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,064:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,065:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,065:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,066:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,067:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,068:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,069:WARNING:findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif
2024-09-04 13:42:31,139:INFO:Visual Rendered Successfully
2024-09-04 13:42:31,607:INFO:plot_model() successfully completed......................................
